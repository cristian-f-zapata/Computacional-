{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPt5BHTwy_0F"
   },
   "source": [
    "# Simple Feature Engineering\n",
    "\n",
    "Preprocessing is often required in ML projects because the raw data is not yet in a suitable format for training a model. Not doing so usually results in the model not converging or having poor performance. Some standard transformations include normalizing pixel values, bucketizing, one-hot encoding, and the like. Consequently, these same transformations should also be done during inference to ensure that the model is computing the correct predictions.\n",
    "\n",
    "1. Collect raw data\n",
    "2. Define metadata\n",
    "3. Create a preprocessing function\n",
    "4. Generate a constant graph with the required transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K4QXVIM7iglN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n",
      "TFX Transform version: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import pprint\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'TFX Transform version: {tft.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOxaaOYRfl7"
   },
   "source": [
    "## Collect raw data\n",
    "\n",
    "For simplicity, we will use dummy data so you can inspect the transformations more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-R236Tkf_ON3"
   },
   "outputs": [],
   "source": [
    "# define sample data\n",
    "raw_data = [\n",
    "      {'x': 1, 'y': 1, 's': 'hello'},\n",
    "      {'x': 2, 'y': 2, 's': 'world'},\n",
    "      {'x': 3, 'y': 3, 's': 'hello'}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metadata\n",
    "\n",
    "Next, we will define the metadata. This contains the schema. **Take note**:\n",
    "\n",
    "* The transform function later expects the metadata to be packed in a [DatasetMetadata](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py#L23) object. \n",
    "* The constructor for the `DatasetMetadata` class expects a [Schema protocol buffer](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L46) data type. You can use the [schema_from_feature_spec()](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py#L36) method to generate that from a dictionary.\n",
    "* To build the said dictionary, you will use the keys/column names of `raw_data` and assign a [FeatureSpecType](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/common_types.py#L29) as values. This allows you to specify if the input is fixed or variable length (using [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) classes), as well as to define the shape and data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema as a DatasetMetadata object\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    \n",
    "    # use convenience function to build a Schema protobuf\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        \n",
    "        # define a dictionary mapping the keys to its feature spec type\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"s\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"x\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"y\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview the schema\n",
    "print(raw_data_metadata._schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zadh6MXLS3eD"
   },
   "source": [
    "## Create  preprocessing function\n",
    "The _preprocessing function_ is the most important concept of `tf.Transform`. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor'><code>Tensor</code></a> or <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor'><code>SparseTensor</code></a>. There are two main groups of API calls that typically form the heart of a preprocessing function:\n",
    "\n",
    "1. **TensorFlow Ops:** Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.\n",
    "2. **TensorFlow Transform Analyzers:** Any of the analyzers provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow ops they only run once during training, and typically make a full pass over the entire training dataset. They create <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/constant'>tensor constants</a>, which are added to your graph. For example, `tft.min` computes the minimum of a tensor over the training dataset.\n",
    "\n",
    "*Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.*\n",
    "\n",
    "You can see available functions to transform your data [here](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H2wANNF_2dCR"
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    \n",
    "    # extract the columns and assign to local variables\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    # data transformations using tft functions\n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "    \n",
    "    # return the transformed data\n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        's_integerized': s_integerized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSl9qyTCbBKR"
   },
   "source": [
    "## Generate a constant graph with the required transformations\n",
    "\n",
    "Tensorflow Transform also uses [Apache Beam](https://beam.apache.org/) for deployment scalability and flexibility. Beam uses the pipe (`|`) operator to stack the different stages of the pipeline. In this case, we will just feed the data (and metadata) to the [AnalyzeAndTransformDataset](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) class and use the preprocessing function you defined above to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mAF9w7RTZU7c"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/home/jovyan/.local/share/jupyter/runtime/kernel-f91dd774-9cae-4d7f-b0af-aa8cf7b3d1ad.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "[{'s': 'hello', 'x': 1, 'y': 1},\n",
      " {'s': 'world', 'x': 2, 'y': 2},\n",
      " {'s': 'hello', 'x': 3, 'y': 3}]\n",
      "\n",
      "Transformed data:\n",
      "[{'s_integerized': 0,\n",
      "  'x_centered': -1.0,\n",
      "  'x_centered_times_y_normalized': -0.0,\n",
      "  'y_normalized': 0.0},\n",
      " {'s_integerized': 1,\n",
      "  'x_centered': 0.0,\n",
      "  'x_centered_times_y_normalized': 0.0,\n",
      "  'y_normalized': 0.5},\n",
      " {'s_integerized': 0,\n",
      "  'x_centered': 1.0,\n",
      "  'x_centered_times_y_normalized': 1.0,\n",
      "  'y_normalized': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# a temporary directory is needed when analyzing the data\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    \n",
    "    # define the pipeline using Apache Beam syntax\n",
    "    transformed_dataset, transform_fn = (\n",
    "        \n",
    "        # analyze and transform the dataset using the preprocessing function\n",
    "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn)\n",
    "    )\n",
    "\n",
    "# unpack the transformed dataset\n",
    "transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "# print the results\n",
    "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n",
    "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "L1_TFX_Transform.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1wmV2p-B2wcWKOTOmImfj6OZgVxWQgbp-",
     "timestamp": 1601072576594
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
